{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 1. Understanding CNNs: The Basics\n","\n","**What are Convolutional Neural Networks?**\n","\n","CNNs are a specialized type of neural network designed for processing data with a grid-like structure, such as images. While humans can easily recognize patterns in images, computers see images as arrays of numbers. CNNs help bridge this gap.\n","\n","**Why use CNNs for images?**\n","\n","Regular neural networks don't work well for images because:\n","\n","\n","*   Images have a lot of pixels (e.g., a 28×28 image has 784 pixels)\n","*   Spatial relationships between pixels matter\n","*    The same pattern can appear in different locations\n","\n","**CNNs solve these problems using three key ideas:**\n","\n","1.   Local connectivity: Looking at small regions of the image at a time\n","2. Parameter sharing: Using the same filters across the entire image\n","3. Pooling: Reducing the size of the representations"],"metadata":{"id":"Jwu8hNCrifxr"}},{"cell_type":"markdown","source":["### The Building Blocks of a CNN"],"metadata":{"id":"K3ArbuPGjcYp"}},{"cell_type":"markdown","source":["#### 1. Convolutional Layer\n","\n","This layer applies filters to the input image to extract features:\n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28010254/conv1.png)\n","\n","How it works:\n","\n","A filter (or **kernel**) slides across the image\n","At each position, it performs element-wise multiplication and summation\n","This creates a feature map highlighting patterns like edges, textures, etc.\n","\n","Here's an animation showing how the filter moves:\n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28011851/conv.gif)\n","\n","#### Key Concepts in Convolution:\n","\n","(Stride, Padding, Filters)\n","\n","* Stride: How many pixels the filter moves each time\n","\n","  * Stride of 1: Move one pixel at a time\n","  * Stride of 2: Move two pixels at a time (reduces output size)\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28090227/stride1.gif)\n","\n","* Padding: Adding pixels (usually zeros) around the image border\n","\n","  * \"Valid\" padding: No padding added, output is smaller than input\n","  * \"Same\" padding: Padding added to keep the output the same size as input\n","\n","  ![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28094927/padding.gif)\n","\n","* Filters: Each filter extracts different features (edges, colors, textures)\n","\n","  * Multiple filters create multiple feature maps\n","  * These maps are stacked to form the output volume\n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28113904/activation-map.png)"],"metadata":{"id":"tuKXWTAzpHLA"}},{"cell_type":"markdown","source":["#### 2. Activation Function (ReLU)\n","\n","After convolution, an activation function is applied to introduce non-linearity. The most common is ReLU (Rectified Linear Unit), which simply replaces negative values with zero.\n","\n","![](https://www.researchgate.net/publication/319235847/figure/fig3/AS:537056121634820@1505055565670/ReLU-activation-function.png)\n"],"metadata":{"id":"XTKs-HUbncgC"}},{"cell_type":"markdown","source":["#### 3. Pooling Layer\n","\n","Pooling reduces the spatial dimensions (width and height) of the feature maps:\n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/06/28022816/maxpool.png)\n","\n","**Types of pooling:**\n","\n","* Max Pooling: Takes the maximum value from each region\n","* Average Pooling: Takes the average value from each region\n","\n","**Benefits of pooling:**\n","\n","* Reduces computation in the network"],"metadata":{"id":"HN3I9a8unfk3"}},{"cell_type":"markdown","source":["#### 4. Fully Connected (Dense) Layers\n","\n","After several convolution and pooling layers, the network uses fully connected layers to:\n","\n","Flatten the 2D feature maps into a 1D vector\n","Combine features to make predictions\n","Output the final classification probabilities"],"metadata":{"id":"uCKYij9kprSu"}},{"cell_type":"markdown","source":["### **The Complete CNN Architecture**\n","\n","A typical CNN has this structure:\n","\n","1. Input Layer (image)\n","2. Convolution Layer + ReLU\n","3. Pooling Layer\n","4. (Repeat steps 2-3 several times)\n","5. Flatten Layer\n","6. Fully Connected (Dense) Layer + ReLU\n","7. Output Layer (with Softmax for classification)"],"metadata":{"id":"dBfthuPzp-Ig"}},{"cell_type":"markdown","source":["# 2. The Fashion MNIST Dataset\n","\n","Before we build our model, let's understand our data.\n","\n","**What is Fashion MNIST?**\n","Fashion MNIST is a dataset of Zalando's fashion article images:\n","\n","* 60,000 training images\n","* 10,000 test images\n","* 28×28 grayscale images\n","* 10 clothing categories\n","\n","It was created as a more challenging drop-in replacement for the original MNIST digits dataset.\n","\n","**The 10 Classes:**\n","\n","Each image belongs to one of these classes:\n","\n","* 0 T-shirt/top\n","* 1 Trouser\n","* 2 Pullover\n","* 3 Dress\n","* 4 Coat\n","* 5 Sandal\n","* 6 Shirt\n","* 7 Sneaker\n","* 8 Bag\n","* 9 Ankle boot\n"],"metadata":{"id":"2s1oOMIMqJlA"}},{"cell_type":"markdown","source":["# 3. Building Our CNN: Step by Step\n","\n","Now let's implement a CNN to classify Fashion MNIST images. We'll break it down into 7 clear steps.\n","\n"],"metadata":{"id":"EPsTRinOxf4y"}},{"cell_type":"markdown","source":["### Step 1: Import Libraries"],"metadata":{"id":"1fqBQ1qY1FR3"}},{"cell_type":"code","source":["# Basic data manipulation and visualization\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Machine learning tools\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","# Deep learning tools\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","# loading the dataset\n","from tensorflow.keras.datasets import fashion_mnist"],"metadata":{"id":"rZbZfgNNndcQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 2: Load and Explore the Data"],"metadata":{"id":"ho_7zizfxuw4"}},{"cell_type":"code","source":["# Load the Fashion MNIST dataset\n","(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n","\n","# Print the shapes to verify\n","print(f\"Training data shape: {X_train.shape}\")\n","print(f\"Training labels shape: {y_train.shape}\")\n","print(f\"Test data shape: {X_test.shape}\")\n","print(f\"Test labels shape: {y_test.shape}\")"],"metadata":{"id":"YP2DDz3w14fk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The data has 785 columns:\n","\n","* The first column contains the class labels (0-9)\n","* The remaining 784 columns contain pixel values (0-255) for the 28×28 image\n","\n","Let's visualize some example images:"],"metadata":{"id":"6xuGTjuSx2O6"}},{"cell_type":"code","source":["# Define class names for better understanding\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","# Display some sample images with their labels\n","plt.figure(figsize=(10, 10))\n","for i in range(25):  # Show 5x5 grid of images\n","    plt.subplot(5, 5, i + 1)\n","    # X_train already contains the image data in shape (samples, 28, 28)\n","    img = X_train[i]  # No need to reshape as it's already in the right format\n","    plt.imshow(img, cmap='gray')\n","    label_idx = y_train[i]  # Get the label directly from y_train\n","    plt.title(class_names[int(label_idx)])\n","    plt.axis('off')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"HYxJtjELyA2E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 3: Prepare the Data"],"metadata":{"id":"bFxMG8xGyABg"}},{"cell_type":"code","source":["# Split the training data into training and validation sets (80% train, 20% validation)\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=0.2, random_state=42)\n","\n","# Print the shapes to verify\n","print(f\"Training data shape: {X_train.shape}\")\n","print(f\"Training labels shape: {y_train.shape}\")\n","print(f\"Validation data shape: {X_val.shape}\")\n","print(f\"Validation labels shape: {y_val.shape}\")\n","print(f\"Test data shape: {X_test.shape}\")\n","print(f\"Test labels shape: {y_test.shape}\")"],"metadata":{"id":"kY0cDpXd3jqp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 4: Build the CNN Model\n"],"metadata":{"id":"Idj0oXntyDk7"}},{"cell_type":"code","source":["# Create a Sequential model (layers are stacked sequentially)\n","model = Sequential([\n","    # First Convolutional Layer\n","    # 32 filters of size 3x3, ReLU activation\n","    Conv2D(\n","        filters=32,          # Number of filters\n","        kernel_size=3,       # Filter size\n","        activation='relu',   # Activation function\n","        input_shape=(28, 28, 1)  # Input image dimensions (height, width, channels)\n","    ),\n","\n","    # First Pooling Layer\n","    # Reduces spatial dimensions by half\n","    MaxPooling2D(pool_size=2),  # 2x2 pooling window\n","\n","    # Dropout Layer (prevents overfitting by randomly \"dropping\" 20% of neurons)\n","    Dropout(0.2),\n","\n","    # Flatten Layer (convert 2D feature maps to 1D feature vector)\n","    Flatten(),\n","\n","    # Fully Connected Layer with 32 neurons\n","    Dense(32, activation='relu'),\n","\n","    # Output Layer with 10 neurons (one for each class)\n","    # Softmax ensures outputs sum to 1 (probability distribution)\n","    Dense(10, activation='softmax')\n","])\n","\n","# Display model summary\n","model.summary()"],"metadata":{"id":"iesrmXgyyG43"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 5: Compile the Model\n"],"metadata":{"id":"y3k9B9LTyLF2"}},{"cell_type":"code","source":["model.compile(\n","    # Loss function measures how well the model is performing\n","    loss='sparse_categorical_crossentropy',  # Appropriate for integer labels\n","\n","    # Optimizer updates model weights based on loss\n","    optimizer=Adam(learning_rate=0.001),     # Adam is an adaptive optimizer\n","\n","    # Metrics to monitor during training\n","    metrics=['accuracy']                     # Percentage of correctly classified images\n",")"],"metadata":{"id":"vTZqiEDuyNTQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 6: Train the Model\n"],"metadata":{"id":"P4twIQmYyRua"}},{"cell_type":"code","source":["# Define batch size and number of epochs\n","batch_size = 128  # Number of samples processed before updating weights\n","epochs = 2       # Number of complete passes through the training dataset\n","\n","# Train the model\n","history = model.fit(\n","    X_train,                  # Training data\n","    y_train,                  # Training labels\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    verbose=1,                # Progress display mode\n","    validation_data=(X_val, y_val)  # Data to evaluate model after each epoch\n",")"],"metadata":{"id":"E0KH6sAfySVd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Step 7: Evaluate the Model\n"],"metadata":{"id":"OKib67uDyY2t"}},{"cell_type":"code","source":["# Evaluate on test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f'Test accuracy: {test_accuracy:.4f}')\n","print(f'Test loss: {test_loss:.4f}')\n","\n","# Visualize training history\n","plt.figure(figsize=(12, 4))\n","\n","# Plot training & validation accuracy\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'], label='Training')\n","plt.plot(history.history['val_accuracy'], label='Validation')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.legend()\n","\n","# Plot training & validation loss\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'], label='Training')\n","plt.plot(history.history['val_loss'], label='Validation')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"6lMVp-73yUMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get predictions for test data\n","y_pred = model.predict(X_test)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","# Create a classification report\n","print(\"Classification Report:\")\n","print(classification_report(\n","    y_test,\n","    y_pred_classes,\n","    target_names=class_names\n","))\n","\n","# Visualize some predictions\n","plt.figure(figsize=(12, 12))\n","for i in range(25):  # 5x5 grid\n","    plt.subplot(5, 5, i + 1)\n","    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n","\n","    # Green title for correct predictions, red for incorrect\n","    actual = int(y_test[i])\n","    predicted = y_pred_classes[i]\n","\n","    if actual == predicted:\n","        color = 'green'\n","    else:\n","        color = 'red'\n","\n","    plt.title(f\"A: {class_names[actual]}\\nP: {class_names[predicted]}\",\n","              color=color)\n","    plt.axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"berMbCgryhLn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Improving the Model\n","\n","Here are some ways to potentially improve our CNN:\n","\n","1. Add more convolutional layers to extract more complex features\n","2. Increase the number of filters in each convolutional layer\n","3. Use data augmentation to artificially expand the training dataset\n","4. Try different optimizers or learning rates\n","5. Experiment with batch normalization to stabilize training\n","\n","Example of a deeper CNN:"],"metadata":{"id":"0KprSwlDytKw"}},{"cell_type":"code","source":["improved_model = Sequential([\n","    # First convolutional block\n","    Conv2D(32, 3, activation='relu', padding='same', input_shape=(28, 28, 1)),\n","    Conv2D(32, 3, activation='relu', padding='same'),\n","    MaxPooling2D(2),\n","    Dropout(0.25),\n","\n","    # Second convolutional block\n","    Conv2D(64, 3, activation='relu', padding='same'),\n","    Conv2D(64, 3, activation='relu', padding='same'),\n","    MaxPooling2D(2),\n","    Dropout(0.25),\n","\n","    # Fully connected layers\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.5),\n","    Dense(10, activation='softmax')\n","])"],"metadata":{"id":"-1ZKztTpy2eo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Conclusion\n","\n","To summarize what we've learned:\n","\n","1. CNNs use specialized layers (convolution, pooling) to effectively process image data\n","2. The convolution operation extracts features using filters that slide across the image\n","3. Pooling reduces dimensions while preserving important information\n","4. The training process involves forward passes to make predictions and backward passes to update weights\n","5. Model evaluation requires separate validation and test sets"],"metadata":{"id":"OjxhvYQRy8SI"}}]}